# DSC2019

네이버 DSC2019 제출용으로 만들었습니다. 분석방법은 올려놓았으나, 데이터는 올려놓지 않았습니다.

## 예선 과제


1. 중간에 결측치가 많아서 힘들었다.
- 네이버에서 제공해준 3일 간격의 데이터에 중간에 누락값이 많아서 어떻게 처리할지 고민을 많이 했다.
- 좀 더 자세한 측정을 위해서 데이터를 최대한 나누는 중, pandas의 interpolate이라는 함수가 선형으로 결측값을 보강하는 역할을 하여 3일 주기 결측값들은 선형으로 채워넣었다.

2. 주제 정하는게 힘들었다.
- 금융 데이터로 독창적이고 신선하게라는걸 고려하느라 시작을 못해서 많이 초초했다.
- 대부분의 금융데이터는 가치(돈)로 연결되서 많이 분석이 되어있는 상태였고, 창의적인걸 고민하기 위해 금융데이터에 대한 정보를 오랜시간동안 찾아다녔다.(주로 사회와 많이 연결 되어 있음)
- 시각화에 대해서 찾다가 지도데이터+히트맵이 쉽게 한눈에 들어올 거 같아서 초점을 잡고 찾다보니 지역별 유가가격을 이용하면 국내 지도를 시각화로 쓸 수 있을 거 같아 선택하엿다.

3. 초기에 판다스 사용법이 익숙하지 않아서 엑셀하고 많이 왔다갔다 함.

4. 데이터 시각화에 대해서.
- datastudio라는 구글에서 공개적으로 제공하는 웹기반 시각화 툴을 유용하게 사용함. 처음에 어떻게 다뤄야 하는지 많이 헤맴. 특히 자료 인코딩... UTF-8로 저장하고 업로드하는데 한글이 자꾸 깨진게 힘들었다.
- 아무래도 툴이다 보니까 제한되는게 많았는데 특히 상관계수 히트맵 같은 경우 원하는대로 나오지 않아 직접  python의 seaborn를 이용해 만들었다.
- 지도 데이터는 SGIS플러스라는 개방형 플랫폼을 이용하여 손쉽게 사용할 수 있었다.

5. 분석 환경
- 주로 파이썬3을 사용하였고 아톰에 hydrogen을 이용해 쥬피터처럼 사용하였다.
- 데이터 관리는 pandas와 엑셀을 이용하였으며, 시각화 툴로 datastudio, SGIS플러스 플랫폼, seaborn, 을 사용하였다.

6. 다른 사람은 어떻게 했을지 하면서 너무 궁금.
- 참가자들은 어떤 독창적인걸 했을지..

7. 분석방법에 대하여
- 정규화로 상관관계까지는 시각화 해서 봤는데, 상관관계를 비교하려면 어떻게 해야되는지 막 여러 상관관계 기법들을 찾아 댕김. AR 이라든지 ... 아직도 잘 이해 안감  
- Cross-correlation  상관 관계 분석을 하려고 하는데 수학적인 식을 잘 못읽어서 여러 블로그를 찾아 다녔다.
- https://www.kaggle.com/dedecu/cross-correlation-time-lag-with-pandas
- 누군가 카글에 Cross-correlation 를 이용해 분석한 방법을 올려 놓았는데 도움이 많이 되엇다.

-----------------------------------------------------------
#분석 순서

1. 1차로 가공된 데이터들을 준비 한다.
- local_oil.csv: 주유소 지역별 평균 판매가격을 csv 로 저장
- st_data_oilForeign.tsv: 제공 받은 해외 유가데이터
- st_data_oilKorea.tsv: 제공 받은 국내 유가데이터

2. st_data_oilKorea.tsv 파일을 일주일 단위로 가공 한뒤 weekly 폴더에 저장한다. (weekly.py 사용)

3. 주간 단위로 저장된 평균 해외유가와 국내 유가를 가져와서 정규화한다.(normalized_weekly.py 사용)
- 날짜 조절(아직 만들지 않음)

4. ccf.py 에서 교차상관분석을 할 수 있다 .